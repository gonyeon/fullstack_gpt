{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    model = \"gpt-4o-mini-2024-07-18\",\n",
    ")\n",
    "\n",
    "filename = 'document.txt'\n",
    "\n",
    "loader = UnstructuredFileLoader(f'../{filename}')\n",
    "cache_dir = LocalFileStore('../cache/')\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator = '\\n\\n',\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100,\n",
    ")\n",
    "\n",
    "\n",
    "docs = loader.load_and_split(text_splitter = splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "memory = ConversationBufferMemory(\n",
    "    llm = llm,\n",
    "    # max_token_limit = 120,\n",
    "    memory_key = 'history',\n",
    "    return_messages = True,\n",
    ")\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\"\"\"\n",
    "            당신은 문서를 받고, 그 문서를 파악하여 대답해주는 AI에요.\n",
    "            만약 해당하는 정보를 문서에서 찾지 못하는 경우엔 '모르겠습니다.' 라고 답변해주세요.\n",
    "            무슨 일이 있어도 한국어로만 대답해주세요.\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": RunnableLambda(load_memory),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke(question).content\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result},\n",
    "    )\n",
    "    print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서에서는 Aaronson이 유죄라고 언급되고 있습니다. 그가 범죄를 저질렀다고 주장되며, 그에 대한 사진이 존재하지 않았고, 그는 그것을 발명했다고 말하고 있습니다.\n",
      "그는 테이블에 \"2+2=5\"라는 메시지를 썼습니다.\n",
      "Julia는 문서에서 Winston의 사랑하는 사람으로 언급됩니다. 그녀는 Winston과 함께 있었던 과거의 연인으로, 그들의 관계는 당국에 의해 위협받고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Aaronson 은 유죄인가요?\")\n",
    "invoke_chain('그가 테이블에 어떤 메시지를 썼나요?')\n",
    "invoke_chain('Julia 는 누구인가요?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
